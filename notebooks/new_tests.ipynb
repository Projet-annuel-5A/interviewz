{
 "cells": [
  {
   "cell_type": "code",
   "id": "5755cf0ba496568e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T01:41:19.992566Z",
     "start_time": "2024-07-08T01:41:14.521037Z"
    }
   },
   "source": [
    "import io\n",
    "import os\n",
    "import av\n",
    "import cv2\n",
    "import torch\n",
    "import tempfile\n",
    "import requests\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepface import DeepFace\n",
    "from typing import List, Dict\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as f\n",
    "from supabase import create_client, Client\n",
    "from transformers import (AutoModelForAudioClassification, Wav2Vec2FeatureExtractor)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dalex\\anaconda3\\envs\\PA5_test\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d1f671b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T01:41:20.001612Z",
     "start_time": "2024-07-08T01:41:19.993571Z"
    }
   },
   "source": [
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "0bed6ecb",
   "metadata": {},
   "source": [
    "CONFIG VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09c9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 14\n",
    "interview_id = 24\n",
    "bucket_name = 'interviews'\n",
    "\n",
    "supabase_client = create_client('https://kglmfklezrjwfvtcolgb.supabase.co', os.environ.get('SUPABASE_KEY'))\n",
    "supabase: Client = create_client('https://kglmfklezrjwfvtcolgb.supabase.co', os.environ.get('SUPABASE_KEY'))\n",
    "supabase_connection = supabase_client.storage.from_('interviews')\n",
    "\n",
    "DIARIZATION_API_URL = 'https://transcribe.whisperapi.com'\n",
    "STT_API_URL = 'https://api.lemonfox.ai/v1/audio/transcriptions'\n",
    "\n",
    "headers = {\n",
    "  'Authorization': 'Bearer {}'.format(os.environ.get('WHISPER_API_KEY'))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524ef1a",
   "metadata": {},
   "source": [
    "Current method save_results_to_bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "87425a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_bd(results: pd.DataFrame) -> None:\n",
    "    try:\n",
    "        response = supabase.table('interviews').select('user_id').eq('id', interview_id).execute()\n",
    "        user_id = response.data[0]['user_id']\n",
    "\n",
    "        results['interview_id'] = interview_id\n",
    "        results['user_id'] = user_id\n",
    "        results = results.fillna('')\n",
    "\n",
    "        data_to_insert = results.to_dict(orient='records')\n",
    "\n",
    "        response = supabase.table('results').insert(data_to_insert).execute()\n",
    "        print('{} lines saved to the database successfully'. format(len(response.data)))\n",
    "    except Exception as e:\n",
    "        print('Error saving results to the database', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df2dcc",
   "metadata": {},
   "source": [
    "Current method save df to bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "97d579b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_bd(results: pd.DataFrame) -> None:\n",
    "    try:\n",
    "        response = supabase.table('interviews').select('user_id').eq('id', interview_id).execute()\n",
    "        user_id = response.data[0]['user_id']\n",
    "\n",
    "        results['interview_id'] = interview_id\n",
    "        results['user_id'] = user_id\n",
    "        results = results.fillna('')\n",
    "\n",
    "        data_to_insert = results.to_dict(orient='records')\n",
    "\n",
    "        response = supabase.table('results').insert(data_to_insert).execute()\n",
    "        print('{} lines saved to the database successfully'. format(len(response.data)))\n",
    "    except Exception as e:\n",
    "        print('Error saving results to the database', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29904869",
   "metadata": {},
   "source": [
    "Current method open_input_file (to modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d297d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize(audio: bytes) -> pd.DataFrame:\n",
    "    file = {'file': io.BytesIO(audio)}\n",
    "    data = {'num_speakers': '2',\n",
    "            'language': 'french',\n",
    "            'diarization': 'true',\n",
    "            'task': 'transcribe',\n",
    "            }\n",
    "    headers = {'Authorization': 'Bearer {}'.format(os.environ.get('WHISPER_API_KEY'))}\n",
    "\n",
    "    response = requests.post(DIARIZATION_API_URL, headers=headers, data=data, files=file)\n",
    "    df = pd.DataFrame(response.json()['diarization'])\n",
    "    \n",
    "    df.rename(columns={'startTime': 'start', 'stopTime': 'end'}, inplace=True)\n",
    "    df['start'] = df['start'].map(lambda x: int(x * 1000))\n",
    "    df['end'] = df['end'].map(lambda x: int(x * 1000))\n",
    "    df['speaker'] = df['speaker'].map(lambda x: int(x.split('_')[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fb80e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(audio_bytes: bytes, diarization: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format=\"mp3\")\n",
    "    headers = {'Authorization': 'Bearer {}'.format(os.environ.get('WHISPER_API_KEY'))}\n",
    "    data = {'model': 'whisper-1',\n",
    "            'language': 'fr',\n",
    "            'response_format': 'text'\n",
    "            }\n",
    "    \n",
    "    for row in diarization.itertuples():\n",
    "        audio_segment = audio[row.start:row.end]\n",
    "        audio_segment_bytes = io.BytesIO()\n",
    "        audio_segment.export(audio_segment_bytes, format=\"mp3\")\n",
    "        audio_segment_bytes.seek(0)\n",
    "        \n",
    "        file = {'file': audio_segment_bytes}\n",
    "        response = requests.post(STT_API_URL, headers=headers, files=file, data=data)\n",
    "        if response.status_code == 200:\n",
    "            diarization.at[row.Index, 'text'] = response.json()\n",
    "            \n",
    "    return diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a40f526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = '{}/{}/raw/raw.mp3'.format(session_id, interview_id)\n",
    "audio_bytes = supabase_connection.download(s3_path)\n",
    "\n",
    "diarization = diarize(audio_bytes)\n",
    "df_full = speech_to_text(audio_bytes, diarization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "67254ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314</td>\n",
       "      <td>4083</td>\n",
       "      <td>1</td>\n",
       "      <td>Première question qu'on nous a énormément posé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4694</td>\n",
       "      <td>6833</td>\n",
       "      <td>1</td>\n",
       "      <td>c'est comment on s'est rencontrés ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7359</td>\n",
       "      <td>8904</td>\n",
       "      <td>1</td>\n",
       "      <td>où on s'est rencontrés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9821</td>\n",
       "      <td>12368</td>\n",
       "      <td>0</td>\n",
       "      <td>Alors moi, je vivais à Montréal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12792</td>\n",
       "      <td>19516</td>\n",
       "      <td>0</td>\n",
       "      <td>Donc on était en 2016, donc de 2016 à 2017 je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20093</td>\n",
       "      <td>21825</td>\n",
       "      <td>0</td>\n",
       "      <td>Et j'ai eu une opportunité.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22351</td>\n",
       "      <td>25899</td>\n",
       "      <td>0</td>\n",
       "      <td>de revenir en France pour travailler.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27003</td>\n",
       "      <td>31519</td>\n",
       "      <td>0</td>\n",
       "      <td>Donc je quitte Montréal, on est en août 2017.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32113</td>\n",
       "      <td>35747</td>\n",
       "      <td>0</td>\n",
       "      <td>et je quitte Montréal pour venir travailler à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36239</td>\n",
       "      <td>38225</td>\n",
       "      <td>0</td>\n",
       "      <td>dans une entreprise, dans une start-up,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38650</td>\n",
       "      <td>39278</td>\n",
       "      <td>0</td>\n",
       "      <td>à Paris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40382</td>\n",
       "      <td>46918</td>\n",
       "      <td>0</td>\n",
       "      <td>Donc, mon premier jour de travail arrive et j'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>47869</td>\n",
       "      <td>51044</td>\n",
       "      <td>0</td>\n",
       "      <td>entre plusieurs personnes mais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>51434</td>\n",
       "      <td>53047</td>\n",
       "      <td>0</td>\n",
       "      <td>C'est celle qui me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53760</td>\n",
       "      <td>55458</td>\n",
       "      <td>0</td>\n",
       "      <td>qui me captivent le plus, on va dire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55458</td>\n",
       "      <td>58921</td>\n",
       "      <td>1</td>\n",
       "      <td>Donc moi, je travaille dans cette entreprise d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>59974</td>\n",
       "      <td>63132</td>\n",
       "      <td>1</td>\n",
       "      <td>Et voilà, quand Mathieu est arrivé au début...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>63522</td>\n",
       "      <td>64847</td>\n",
       "      <td>1</td>\n",
       "      <td>C'est pas spécialement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>65747</td>\n",
       "      <td>67325</td>\n",
       "      <td>1</td>\n",
       "      <td>parler, pas plus que ça</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>68140</td>\n",
       "      <td>72351</td>\n",
       "      <td>1</td>\n",
       "      <td>Et c'est vrai que quelques années plus tard, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72877</td>\n",
       "      <td>75933</td>\n",
       "      <td>1</td>\n",
       "      <td>à discuter on parlait de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>76494</td>\n",
       "      <td>78361</td>\n",
       "      <td>1</td>\n",
       "      <td>de sujets qui nous passionnent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78938</td>\n",
       "      <td>79601</td>\n",
       "      <td>1</td>\n",
       "      <td>tous les deux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>80144</td>\n",
       "      <td>81587</td>\n",
       "      <td>1</td>\n",
       "      <td>C'est vrai qu'on aimait beaucoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>82470</td>\n",
       "      <td>83268</td>\n",
       "      <td>1</td>\n",
       "      <td>une émission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>83879</td>\n",
       "      <td>84898</td>\n",
       "      <td>1</td>\n",
       "      <td>qui s'appelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>85101</td>\n",
       "      <td>87852</td>\n",
       "      <td>1</td>\n",
       "      <td>Koh-Lanta, c'est l'équivalent français de Surv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>88718</td>\n",
       "      <td>95152</td>\n",
       "      <td>1</td>\n",
       "      <td>Et on échangeait pas mal quand on regardait ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>95628</td>\n",
       "      <td>96816</td>\n",
       "      <td>1</td>\n",
       "      <td>sur les participants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>96901</td>\n",
       "      <td>101757</td>\n",
       "      <td>1</td>\n",
       "      <td>Ça et aussi, Mathieu avait déjà très bien comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>102181</td>\n",
       "      <td>104745</td>\n",
       "      <td>1</td>\n",
       "      <td>Et donc ils m'ont envoyé plein de photos migno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>105135</td>\n",
       "      <td>107071</td>\n",
       "      <td>1</td>\n",
       "      <td>Du chien de sa soeur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>107461</td>\n",
       "      <td>114728</td>\n",
       "      <td>0</td>\n",
       "      <td>C'est vrai que j'avais senti qu'il fallait que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>115016</td>\n",
       "      <td>115831</td>\n",
       "      <td>0</td>\n",
       "      <td>de ses goûts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>116595</td>\n",
       "      <td>117750</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour lui plaire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>118089</td>\n",
       "      <td>119685</td>\n",
       "      <td>1</td>\n",
       "      <td>Toi aussi, c'est tes goûts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>119142</td>\n",
       "      <td>120789</td>\n",
       "      <td>0</td>\n",
       "      <td>Oui, moi aussi, j'aime le petit chien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>120789</td>\n",
       "      <td>125662</td>\n",
       "      <td>1</td>\n",
       "      <td>Et donc, oui, on s'est rencontrés comme 14% de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>125933</td>\n",
       "      <td>129176</td>\n",
       "      <td>1</td>\n",
       "      <td>au travail. Moi je pensais que c'était plus. O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>128106</td>\n",
       "      <td>130534</td>\n",
       "      <td>0</td>\n",
       "      <td>14% ça m'a l'air faible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>129906</td>\n",
       "      <td>130382</td>\n",
       "      <td>1</td>\n",
       "      <td>faible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>131332</td>\n",
       "      <td>136341</td>\n",
       "      <td>1</td>\n",
       "      <td>Mais en fait, je me suis un peu renseignée et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>136544</td>\n",
       "      <td>139601</td>\n",
       "      <td>1</td>\n",
       "      <td>mais ça n'a pas forcément été des histoires qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>139872</td>\n",
       "      <td>142130</td>\n",
       "      <td>1</td>\n",
       "      <td>Là, les 14%, c'est vraiment des gens qui sont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>143471</td>\n",
       "      <td>145577</td>\n",
       "      <td>1</td>\n",
       "      <td>Encore aujourd'hui en couple, ils ont répondu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>145916</td>\n",
       "      <td>146816</td>\n",
       "      <td>1</td>\n",
       "      <td>au sondage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     start     end  speaker                                               text\n",
       "0      314    4083        1  Première question qu'on nous a énormément posé...\n",
       "1     4694    6833        1                c'est comment on s'est rencontrés ?\n",
       "2     7359    8904        1                            où on s'est rencontrés.\n",
       "3     9821   12368        0                   Alors moi, je vivais à Montréal.\n",
       "4    12792   19516        0  Donc on était en 2016, donc de 2016 à 2017 je ...\n",
       "5    20093   21825        0                        Et j'ai eu une opportunité.\n",
       "6    22351   25899        0              de revenir en France pour travailler.\n",
       "7    27003   31519        0      Donc je quitte Montréal, on est en août 2017.\n",
       "8    32113   35747        0  et je quitte Montréal pour venir travailler à ...\n",
       "9    36239   38225        0            dans une entreprise, dans une start-up,\n",
       "10   38650   39278        0                                           à Paris.\n",
       "11   40382   46918        0  Donc, mon premier jour de travail arrive et j'...\n",
       "12   47869   51044        0                     entre plusieurs personnes mais\n",
       "13   51434   53047        0                              C'est celle qui me...\n",
       "14   53760   55458        0              qui me captivent le plus, on va dire.\n",
       "15   55458   58921        1  Donc moi, je travaille dans cette entreprise d...\n",
       "16   59974   63132        1     Et voilà, quand Mathieu est arrivé au début...\n",
       "17   63522   64847        1                          C'est pas spécialement...\n",
       "18   65747   67325        1                            parler, pas plus que ça\n",
       "19   68140   72351        1  Et c'est vrai que quelques années plus tard, o...\n",
       "20   72877   75933        1                           à discuter on parlait de\n",
       "21   76494   78361        1                     de sujets qui nous passionnent\n",
       "22   78938   79601        1                                     tous les deux.\n",
       "23   80144   81587        1                   C'est vrai qu'on aimait beaucoup\n",
       "24   82470   83268        1                                       une émission\n",
       "25   83879   84898        1                                   qui s'appelle...\n",
       "26   85101   87852        1  Koh-Lanta, c'est l'équivalent français de Surv...\n",
       "27   88718   95152        1  Et on échangeait pas mal quand on regardait ch...\n",
       "28   95628   96816        1                               sur les participants\n",
       "29   96901  101757        1  Ça et aussi, Mathieu avait déjà très bien comp...\n",
       "30  102181  104745        1  Et donc ils m'ont envoyé plein de photos migno...\n",
       "31  105135  107071        1                              Du chien de sa soeur.\n",
       "32  107461  114728        0  C'est vrai que j'avais senti qu'il fallait que...\n",
       "33  115016  115831        0                                       de ses goûts\n",
       "34  116595  117750        0                                   Pour lui plaire.\n",
       "35  118089  119685        1                        Toi aussi, c'est tes goûts.\n",
       "36  119142  120789        0             Oui, moi aussi, j'aime le petit chien.\n",
       "37  120789  125662        1  Et donc, oui, on s'est rencontrés comme 14% de...\n",
       "38  125933  129176        1  au travail. Moi je pensais que c'était plus. O...\n",
       "39  128106  130534        0                           14% ça m'a l'air faible.\n",
       "40  129906  130382        1                                             faible\n",
       "41  131332  136341        1  Mais en fait, je me suis un peu renseignée et ...\n",
       "42  136544  139601        1  mais ça n'a pas forcément été des histoires qu...\n",
       "43  139872  142130        1   Là, les 14%, c'est vraiment des gens qui sont...\n",
       "44  143471  145577        1   Encore aujourd'hui en couple, ils ont répondu...\n",
       "45  145916  146816        1                                         au sondage"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to_bd(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6b29aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Alors moi, je vivais à Montréal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Donc on était en 2016, donc de 2016 à 2017 je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Et j'ai eu une opportunité.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>de revenir en France pour travailler.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>Donc je quitte Montréal, on est en août 2017.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>et je quitte Montréal pour venir travailler à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>dans une entreprise, dans une start-up,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>à Paris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>Donc, mon premier jour de travail arrive et j'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>entre plusieurs personnes mais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>C'est celle qui me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>qui me captivent le plus, on va dire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>C'est vrai que j'avais senti qu'il fallait que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>de ses goûts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>Pour lui plaire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Oui, moi aussi, j'aime le petit chien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>14% ça m'a l'air faible.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "id                                                     \n",
       "1052                   Alors moi, je vivais à Montréal.\n",
       "1053  Donc on était en 2016, donc de 2016 à 2017 je ...\n",
       "1054                        Et j'ai eu une opportunité.\n",
       "1055              de revenir en France pour travailler.\n",
       "1056      Donc je quitte Montréal, on est en août 2017.\n",
       "1057  et je quitte Montréal pour venir travailler à ...\n",
       "1058            dans une entreprise, dans une start-up,\n",
       "1059                                           à Paris.\n",
       "1060  Donc, mon premier jour de travail arrive et j'...\n",
       "1061                     entre plusieurs personnes mais\n",
       "1062                              C'est celle qui me...\n",
       "1063              qui me captivent le plus, on va dire.\n",
       "1081  C'est vrai que j'avais senti qu'il fallait que...\n",
       "1082                                       de ses goûts\n",
       "1083                                   Pour lui plaire.\n",
       "1085             Oui, moi aussi, j'aime le petit chien.\n",
       "1088                           14% ça m'a l'air faible."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = supabase.table('results').select('text', 'id').eq('interview_id', interview_id).eq('speaker', 0).execute()\n",
    "results = pd.DataFrame(res.data)\n",
    "results.set_index('id', inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf0a7a",
   "metadata": {},
   "source": [
    "## AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dd07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_input_file(s3_path: str, file_name: str) -> bytes | None:\n",
    "    try:\n",
    "        print('Getting file {} from the S3 bucket'.format(file_name))\n",
    "        file_bytes = supabase_connection.download(s3_path)\n",
    "        return file_bytes\n",
    "    except Exception as e:\n",
    "        message = ('Error downloading the file {} from the S3 bucket. '.\n",
    "                    format(file_name), str(e))\n",
    "        print(message)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fcb2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments_from_db() -> pd.DataFrame:\n",
    "    res = (supabase.table('results').select('id', 'start', 'end')\n",
    "            .eq('interview_id', interview_id)\n",
    "            .eq('speaker', 0)\n",
    "            .execute())\n",
    "    results = pd.DataFrame(res.data)\n",
    "    results.set_index('id', inplace=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e025510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a3e7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_model = AutoModelForAudioClassification.from_pretrained('Lajavaness/wav2vec2-lg-xlsr-fr-speech-emotion-recognition')\n",
    "ate_model.to(device)\n",
    "ate_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('Lajavaness/wav2vec2-lg-xlsr-fr-speech-emotion-recognition')\n",
    "ate_sampling_rate = ate_feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc80a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/31/raw/raw.mp3\n",
      "Getting file raw.mp3 from the S3 bucket\n"
     ]
    }
   ],
   "source": [
    "filename = 'raw.mp3'\n",
    "s3_path = '{}/{}/raw/{}'.format(session_id, interview_id, filename)\n",
    "print(s3_path)\n",
    "audio_bytes = open_input_file(s3_path, filename)\n",
    "audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7964fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = get_segments_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb5d40db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalex\\anaconda3\\envs\\PA5_test\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:958: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "sentiments = list()\n",
    "\n",
    "for row in segments.itertuples():\n",
    "    audio_segment = audio[row.start:row.end]\n",
    "    audio_segment_bytes = io.BytesIO()\n",
    "    audio_segment.export(audio_segment_bytes, format=\"mp3\")\n",
    "    audio_segment_bytes.seek(0)\n",
    "    \n",
    "    speech_array, sample_rate = torchaudio.load(audio_segment_bytes)\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, ate_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "\n",
    "    inputs = ate_feature_extractor(speech, sampling_rate=ate_sampling_rate,\n",
    "                                                return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = ate_model(**inputs).logits\n",
    "\n",
    "    scores = f.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    # Get the percentage scores and round them to 5 decimal places\n",
    "    scores = [round(num * 100, 5) for num in scores]\n",
    "\n",
    "    # Get a dictionary with the labels for each emotion and its values\n",
    "    values_dict = dict(zip(ate_model.config.id2label.values(), scores))\n",
    "\n",
    "    # Sort the dictionary by values in descending order\n",
    "    sorted_values = {k: v for k, v in sorted(values_dict.items(), key=lambda x: x[1], reverse=True)}\n",
    "    \n",
    "    sentiments.append(sorted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a4620e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>audio_emotions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>9821</td>\n",
       "      <td>12368</td>\n",
       "      <td>{'Neutral': 99.97324, 'Relaxed': 0.01292, 'Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>12792</td>\n",
       "      <td>19516</td>\n",
       "      <td>{'Neutral': 99.99914, 'Relaxed': 0.00026, 'Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>20093</td>\n",
       "      <td>21825</td>\n",
       "      <td>{'Neutral': 99.99955, 'Sad': 0.00025, 'Tension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>22351</td>\n",
       "      <td>25899</td>\n",
       "      <td>{'Neutral': 99.99902, 'Pleased': 0.00031, 'Rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>27003</td>\n",
       "      <td>31519</td>\n",
       "      <td>{'Neutral': 99.99893, 'Sad': 0.00056, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>32113</td>\n",
       "      <td>35747</td>\n",
       "      <td>{'Neutral': 99.99925, 'Sad': 0.00028, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>36239</td>\n",
       "      <td>38225</td>\n",
       "      <td>{'Neutral': 99.99923, 'Sad': 0.00032, 'Tension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>38650</td>\n",
       "      <td>39278</td>\n",
       "      <td>{'Neutral': 99.99903, 'Sad': 0.00039, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>40382</td>\n",
       "      <td>46918</td>\n",
       "      <td>{'Neutral': 99.99934, 'Sad': 0.00019, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>47869</td>\n",
       "      <td>51044</td>\n",
       "      <td>{'Neutral': 99.99785, 'Relaxed': 0.00116, 'Sad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>51434</td>\n",
       "      <td>53047</td>\n",
       "      <td>{'Neutral': 95.34285, 'Relaxed': 4.65514, 'Sad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>53760</td>\n",
       "      <td>55458</td>\n",
       "      <td>{'Neutral': 98.78153, 'Sad': 0.62266, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>107461</td>\n",
       "      <td>114728</td>\n",
       "      <td>{'Neutral': 99.39679, 'Relaxed': 0.36604, 'Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>115016</td>\n",
       "      <td>115831</td>\n",
       "      <td>{'Neutral': 99.99952, 'Sad': 0.00024, 'Relaxed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>116595</td>\n",
       "      <td>117750</td>\n",
       "      <td>{'Neutral': 85.16373, 'Tension': 14.54359, 'Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>119142</td>\n",
       "      <td>120789</td>\n",
       "      <td>{'Neutral': 92.91589, 'Tension': 7.03339, 'Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>128106</td>\n",
       "      <td>130534</td>\n",
       "      <td>{'Neutral': 99.94367, 'Sad': 0.03102, 'Relaxed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start     end                                     audio_emotions\n",
       "id                                                                     \n",
       "1052    9821   12368  {'Neutral': 99.97324, 'Relaxed': 0.01292, 'Ple...\n",
       "1053   12792   19516  {'Neutral': 99.99914, 'Relaxed': 0.00026, 'Ple...\n",
       "1054   20093   21825  {'Neutral': 99.99955, 'Sad': 0.00025, 'Tension...\n",
       "1055   22351   25899  {'Neutral': 99.99902, 'Pleased': 0.00031, 'Rel...\n",
       "1056   27003   31519  {'Neutral': 99.99893, 'Sad': 0.00056, 'Relaxed...\n",
       "1057   32113   35747  {'Neutral': 99.99925, 'Sad': 0.00028, 'Relaxed...\n",
       "1058   36239   38225  {'Neutral': 99.99923, 'Sad': 0.00032, 'Tension...\n",
       "1059   38650   39278  {'Neutral': 99.99903, 'Sad': 0.00039, 'Relaxed...\n",
       "1060   40382   46918  {'Neutral': 99.99934, 'Sad': 0.00019, 'Relaxed...\n",
       "1061   47869   51044  {'Neutral': 99.99785, 'Relaxed': 0.00116, 'Sad...\n",
       "1062   51434   53047  {'Neutral': 95.34285, 'Relaxed': 4.65514, 'Sad...\n",
       "1063   53760   55458  {'Neutral': 98.78153, 'Sad': 0.62266, 'Relaxed...\n",
       "1081  107461  114728  {'Neutral': 99.39679, 'Relaxed': 0.36604, 'Ple...\n",
       "1082  115016  115831  {'Neutral': 99.99952, 'Sad': 0.00024, 'Relaxed...\n",
       "1083  116595  117750  {'Neutral': 85.16373, 'Tension': 14.54359, 'Sa...\n",
       "1085  119142  120789  {'Neutral': 92.91589, 'Tension': 7.03339, 'Ple...\n",
       "1088  128106  130534  {'Neutral': 99.94367, 'Sad': 0.03102, 'Relaxed..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments['audio_emotions'] = sentiments\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b3663",
   "metadata": {},
   "source": [
    "## VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00451fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image: np.ndarray) -> Dict[str, float]:\n",
    "    try:\n",
    "        objs = DeepFace.analyze(image, actions=['emotion'])\n",
    "        results = objs[0]['emotion']\n",
    "        emotions = {k: v for k, v in sorted(results.items(), key=lambda x: x[1], reverse=True)}\n",
    "    except ValueError:\n",
    "        emotions = {'No face detected': 0.0}\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702d03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(segments: pd.DataFrame) -> List[List[Dict[str, float]]]:\n",
    "    all_sentiments = list()\n",
    "    \n",
    "    print('Processing sentiments from video')\n",
    "\n",
    "    filename = 'raw.mp4'\n",
    "    s3_path = '{}/{}/raw/{}'.format(session_id, interview_id, filename)\n",
    "    print(s3_path)\n",
    "    video_bytes = open_input_file(s3_path, filename)\n",
    "\n",
    "    with (tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file):\n",
    "        temp_file_path = temp_file.name\n",
    "        try:\n",
    "            temp_file.write(video_bytes)\n",
    "            clip = cv2.VideoCapture(temp_file_path)\n",
    "\n",
    "            # Get video fps\n",
    "            fps = clip.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "            # Set the interval for extracting frames\n",
    "            timing = 2.0\n",
    "            interval = int(fps) * timing\n",
    "\n",
    "            for row in segments.itertuples():\n",
    "                sentiments = list()\n",
    "\n",
    "                # Calculate frame indices for starting and ending times\n",
    "                start_frame = int(row.start / 1000 * fps)\n",
    "                end_frame = int(row.end / 1000 * fps)\n",
    "\n",
    "                # Set starting frame\n",
    "                clip.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "                # Read the video frame by frame and send respective frames to prediction\n",
    "                frame_count = 0\n",
    "                image_count = 0\n",
    "                while clip.isOpened() and frame_count <= (end_frame - start_frame):\n",
    "                    ret, frame = clip.read()\n",
    "\n",
    "                    # If there are no more frames, break the loop\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    # Detect emotions from the frame if it's the first one or if it's a multiple of the interval\n",
    "                    if frame_count == 0 or frame_count % interval == 0:\n",
    "                        image_name = 'image_{:05d}'.format(image_count)\n",
    "                        image_count += 1\n",
    "\n",
    "                        sentiments.append({image_name: predict(frame)})\n",
    "\n",
    "                    # Save the last frame\n",
    "                    elif start_frame + frame_count == end_frame:\n",
    "                        image_name = 'image_{:05d}'.format(image_count)\n",
    "                        image_count += 1\n",
    "\n",
    "                        sentiments.append({image_name: predict(frame)})\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "                all_sentiments.append(sentiments)\n",
    "            \n",
    "            # Release the video capture object\n",
    "            clip.release()\n",
    "        except Exception as e:\n",
    "            message = ('Error processing video emotions.', str(e))\n",
    "            print(message)\n",
    "        finally:\n",
    "            temp_file.close()\n",
    "            # Clean up the temporary file\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "\n",
    "        print('Emotions extraction from video have finished')\n",
    "        return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74bce27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_segments_from_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m segments \u001B[38;5;241m=\u001B[39m \u001B[43mget_segments_from_db\u001B[49m()\n\u001B[0;32m      2\u001B[0m res \u001B[38;5;241m=\u001B[39m process(segments)\n\u001B[0;32m      4\u001B[0m segments[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvideo_emotions\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m res\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_segments_from_db' is not defined"
     ]
    }
   ],
   "source": [
    "segments = get_segments_from_db()\n",
    "res = process(segments)\n",
    "\n",
    "segments['video_emotions'] = res\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fee82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/31/raw/raw.mp4\n",
      "Getting file raw.mp4 from the S3 bucket\n"
     ]
    }
   ],
   "source": [
    "filename = 'raw.mp4'\n",
    "s3_path = '{}/{}/raw/{}'.format(session_id, interview_id, filename)\n",
    "print(s3_path)\n",
    "video_bytes = open_input_file(s3_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc31981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "# Assuming `video_bytes` is your array of bytes\n",
    "container = av.open(io.BytesIO(video_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa7a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "stream = container.streams.video[0]\n",
    "fps = stream.average_rate\n",
    "print(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2144e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 10  # start at 10 seconds\n",
    "end_time = 20    # end at 20 seconds\n",
    "\n",
    "# Seek to the start time\n",
    "container.seek(int(start_time * av.time_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda70b10",
   "metadata": {},
   "source": [
    "## NEW TEST VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f49c2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "timing = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "35d9a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_process(segments: pd.DataFrame) -> List[List[Dict[str, float]]]:\n",
    "    all_sentiments = list()\n",
    "    \n",
    "    print('Processing sentiments from video')\n",
    "\n",
    "    # filename = 'raw.mp4'\n",
    "    # s3_path = '{}/{}/raw/{}'.format(session_id, interview_id, filename)\n",
    "    # video_bytes = open_input_file(s3_path, filename)\n",
    "    \n",
    "    container = av.open(io.BytesIO(video_bytes))\n",
    "    \n",
    "    for row in segments.itertuples():\n",
    "        sentiments = list()\n",
    "        image_count = 0\n",
    "        \n",
    "        start_time = row.start / 1000\n",
    "        end_time = row.end / 1000\n",
    "        \n",
    "        container.seek(int(start_time * av.time_base))\n",
    "        # print('Clip start time:', start_time, 'Clip end time:', end_time)\n",
    "        \n",
    "        # Flag to ensure end_time frame is captured\n",
    "        end_frame_captured = False\n",
    "        \n",
    "        # Initialize variables to track frame extraction\n",
    "        last_extracted_time = start_time - timing  # ensures the first frame is extracted at start_time\n",
    "        \n",
    "        for frame in container.decode(video=0):\n",
    "            frame_time = frame.time\n",
    "        \n",
    "            if frame_time < start_time:\n",
    "                continue\n",
    "            \n",
    "            # Extract the last frame at end_time\n",
    "            if frame_time >= end_time and not end_frame_captured:                \n",
    "                img = frame.to_image()\n",
    "                img_array = np.array(img)\n",
    "                sentiments.append({'frame_{:05d}'.format(image_count): predict(img_array)})\n",
    "                image_count += 1\n",
    "                \n",
    "                end_frame_captured = True\n",
    "                # print('Last frame captured: ', frame_time)\n",
    "                break\n",
    "            \n",
    "            if frame_time >= start_time and frame_time >= last_extracted_time + timing and frame_time < end_time:\n",
    "                img = frame.to_image()\n",
    "                img_array = np.array(img)\n",
    "                sentiments.append({'frame_{:05d}'.format(image_count): predict(img_array)})\n",
    "                image_count += 1\n",
    "                # print('Capturing frame ', frame_time)\n",
    "                \n",
    "                # Update the last extracted time\n",
    "                last_extracted_time = frame_time\n",
    "                   \n",
    "        all_sentiments.append(sentiments)\n",
    "\n",
    "    container.close()\n",
    "    # cv2.destroyAllWindows()\n",
    "    \n",
    "    return all_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "627914dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = get_segments_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1e6fc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentiments from video\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'frame_00000': {'neutral': 78.19947004318237,\n",
       "    'happy': 15.301547944545746,\n",
       "    'fear': 3.052571602165699,\n",
       "    'angry': 1.927490159869194,\n",
       "    'sad': 1.5153182670474052,\n",
       "    'surprise': 0.003532637856551446,\n",
       "    'disgust': 7.304662403839757e-05}},\n",
       "  {'frame_00001': {'neutral': 85.57081818580627,\n",
       "    'sad': 8.659198135137558,\n",
       "    'fear': 3.148382157087326,\n",
       "    'angry': 1.8813705071806908,\n",
       "    'happy': 0.4544760100543499,\n",
       "    'surprise': 0.2855287166312337,\n",
       "    'disgust': 0.0002256660991406534}}],\n",
       " [{'frame_00000': {'sad': 79.05328273773193,\n",
       "    'fear': 14.143165946006775,\n",
       "    'neutral': 6.61165714263916,\n",
       "    'angry': 0.14851256273686886,\n",
       "    'happy': 0.043335300870239735,\n",
       "    'surprise': 4.749843753870664e-05,\n",
       "    'disgust': 7.455372430520413e-08}},\n",
       "  {'frame_00001': {'fear': 75.58532357215881,\n",
       "    'neutral': 19.58324909210205,\n",
       "    'sad': 4.5829176902771,\n",
       "    'angry': 0.23604626767337322,\n",
       "    'surprise': 0.012302663526497781,\n",
       "    'happy': 0.0001535157139187504,\n",
       "    'disgust': 5.575387262979348e-06}},\n",
       "  {'frame_00002': {'fear': 67.69056723657651,\n",
       "    'neutral': 16.400587046181382,\n",
       "    'sad': 12.261570230816021,\n",
       "    'angry': 3.4850577107106275,\n",
       "    'surprise': 0.16006340528284937,\n",
       "    'happy': 0.0020045313534583025,\n",
       "    'disgust': 0.0001523338118962317}}],\n",
       " [{'frame_00000': {'neutral': 89.01121020317078,\n",
       "    'sad': 10.74308454990387,\n",
       "    'fear': 0.1507702749222517,\n",
       "    'happy': 0.0795812055002898,\n",
       "    'angry': 0.01506586850155145,\n",
       "    'surprise': 0.000289528043140308,\n",
       "    'disgust': 4.0819952973336626e-08}},\n",
       "  {'frame_00001': {'happy': 99.99978542327881,\n",
       "    'neutral': 0.00015981669321263325,\n",
       "    'surprise': 5.943483074588585e-05,\n",
       "    'sad': 9.215330755429907e-10,\n",
       "    'angry': 6.023451187700882e-12,\n",
       "    'fear': 5.4375375340196376e-14,\n",
       "    'disgust': 1.1092710599804329e-23}},\n",
       "  {'frame_00002': {'happy': 99.97341037113543,\n",
       "    'neutral': 0.01539063776610267,\n",
       "    'angry': 0.008350382727711471,\n",
       "    'sad': 0.002728815381786908,\n",
       "    'fear': 8.536616866048057e-05,\n",
       "    'surprise': 2.963745588691325e-05,\n",
       "    'disgust': 1.1906658960483517e-08}},\n",
       "  {'frame_00003': {'happy': 99.99951720234854,\n",
       "    'neutral': 0.0004832384835658716,\n",
       "    'surprise': 6.330713553245988e-08,\n",
       "    'sad': 1.0062350164450686e-08,\n",
       "    'fear': 7.037865718712054e-12,\n",
       "    'angry': 4.842920312542837e-12,\n",
       "    'disgust': 1.1104593676289808e-20}},\n",
       "  {'frame_00004': {'happy': 99.99655485194268,\n",
       "    'neutral': 0.0018613390748492427,\n",
       "    'surprise': 0.001578136464695942,\n",
       "    'fear': 8.95354972232356e-07,\n",
       "    'sad': 6.765432740215783e-07,\n",
       "    'angry': 2.696354373644853e-07,\n",
       "    'disgust': 6.197778379151871e-15}}],\n",
       " [{'frame_00000': {'happy': 99.998939037323,\n",
       "    'sad': 0.0005382242306950502,\n",
       "    'neutral': 0.0005119659817864886,\n",
       "    'surprise': 1.5222784099933051e-05,\n",
       "    'angry': 1.1338945338223994e-07,\n",
       "    'fear': 7.556391623531056e-08,\n",
       "    'disgust': 7.829952712084051e-15}},\n",
       "  {'frame_00001': {'surprise': 67.12718410057927,\n",
       "    'fear': 32.72485630064346,\n",
       "    'happy': 0.07132302696320575,\n",
       "    'neutral': 0.051989508399114516,\n",
       "    'sad': 0.01597785689875748,\n",
       "    'angry': 0.008661200051956627,\n",
       "    'disgust': 1.0469858493618309e-05}}],\n",
       " [{'frame_00000': {'sad': 37.795352935791016,\n",
       "    'neutral': 29.159867763519287,\n",
       "    'angry': 21.986469626426697,\n",
       "    'fear': 10.344761610031128,\n",
       "    'happy': 0.2977027790620923,\n",
       "    'surprise': 0.27767703868448734,\n",
       "    'disgust': 0.13817378785461187}},\n",
       "  {'frame_00001': {'fear': 75.1607865286811,\n",
       "    'sad': 10.030226115015282,\n",
       "    'angry': 9.597303372300074,\n",
       "    'neutral': 5.114367468877729,\n",
       "    'surprise': 0.09716955822678605,\n",
       "    'disgust': 0.00015453553738123624,\n",
       "    'happy': 2.2479556897852017e-06}}],\n",
       " [{'frame_00000': {'fear': 85.14131726519075,\n",
       "    'neutral': 11.404476929244629,\n",
       "    'surprise': 1.7772080632696228,\n",
       "    'sad': 1.3257874825045446,\n",
       "    'angry': 0.18941936944208973,\n",
       "    'happy': 0.1615883858719034,\n",
       "    'disgust': 0.00020647403476956668}},\n",
       "  {'frame_00001': {'fear': 70.71561464257415,\n",
       "    'sad': 15.622488036452298,\n",
       "    'angry': 8.45905196178579,\n",
       "    'neutral': 2.227118504299939,\n",
       "    'disgust': 1.9458953748220653,\n",
       "    'surprise': 1.0085338066260585,\n",
       "    'happy': 0.02130194879290596}},\n",
       "  {'frame_00002': {'fear': 69.49542164802551,\n",
       "    'sad': 16.726955771446228,\n",
       "    'angry': 8.43278169631958,\n",
       "    'neutral': 2.636069804430008,\n",
       "    'disgust': 1.4656617306172848,\n",
       "    'surprise': 1.237732358276844,\n",
       "    'happy': 0.005383183088270016}},\n",
       "  {'frame_00003': {'neutral': 84.01151204177809,\n",
       "    'sad': 11.306951868948452,\n",
       "    'angry': 3.069825280894356,\n",
       "    'fear': 1.5874502263933554,\n",
       "    'happy': 0.021039633228477583,\n",
       "    'surprise': 0.002999155182399258,\n",
       "    'disgust': 0.00022340447198169883}},\n",
       "  {'frame_00004': {'neutral': 63.83624076843262,\n",
       "    'sad': 32.76807367801666,\n",
       "    'fear': 2.088497020304203,\n",
       "    'happy': 0.720740295946598,\n",
       "    'angry': 0.583001459017396,\n",
       "    'surprise': 0.003022138298547361,\n",
       "    'disgust': 0.0004259006800566567}}],\n",
       " [{'frame_00000': {'sad': 48.720985651016235,\n",
       "    'neutral': 42.808035016059875,\n",
       "    'angry': 3.6521486937999725,\n",
       "    'fear': 3.4859880805015564,\n",
       "    'happy': 0.9843712672591209,\n",
       "    'disgust': 0.25096856988966465,\n",
       "    'surprise': 0.09749894379638135}},\n",
       "  {'frame_00001': {'neutral': 95.04560173101446,\n",
       "    'sad': 1.762628020361426,\n",
       "    'happy': 1.4057147129732028,\n",
       "    'angry': 1.34111593632292,\n",
       "    'fear': 0.4326166169484397,\n",
       "    'surprise': 0.011982963015389876,\n",
       "    'disgust': 0.00034300687271624777}},\n",
       "  {'frame_00002': {'neutral': 74.2422103881836,\n",
       "    'sad': 19.31823641061783,\n",
       "    'fear': 3.402576968073845,\n",
       "    'angry': 2.879956178367138,\n",
       "    'surprise': 0.13524520909413695,\n",
       "    'happy': 0.02159735740860924,\n",
       "    'disgust': 0.00018256989733345108}}],\n",
       " [{'frame_00000': {'sad': 50.63941478729248,\n",
       "    'fear': 33.05475115776062,\n",
       "    'neutral': 12.49636709690094,\n",
       "    'angry': 2.31221467256546,\n",
       "    'disgust': 1.2237162329256535,\n",
       "    'surprise': 0.21656646858900785,\n",
       "    'happy': 0.05696889711543918}},\n",
       "  {'frame_00001': {'neutral': 88.11871409416199,\n",
       "    'sad': 6.938160955905914,\n",
       "    'fear': 3.9247192442417145,\n",
       "    'angry': 0.9574536234140396,\n",
       "    'happy': 0.03547805536072701,\n",
       "    'surprise': 0.02543579030316323,\n",
       "    'disgust': 4.241741180521785e-05}},\n",
       "  {'frame_00002': {'neutral': 74.82223956717489,\n",
       "    'fear': 11.257295981484413,\n",
       "    'surprise': 5.719588665341655,\n",
       "    'sad': 5.176490108757622,\n",
       "    'happy': 2.0349941498037665,\n",
       "    'angry': 0.9888738983198344,\n",
       "    'disgust': 0.0005177011498090565}}],\n",
       " [{'frame_00000': {'sad': 32.60372371717974,\n",
       "    'angry': 30.827421644618706,\n",
       "    'fear': 28.388533400122334,\n",
       "    'neutral': 7.714352931303135,\n",
       "    'surprise': 0.36972772546592064,\n",
       "    'disgust': 0.08742062369346604,\n",
       "    'happy': 0.008820449471490022}},\n",
       "  {'frame_00001': {'neutral': 88.73705998217345,\n",
       "    'sad': 8.59630377932742,\n",
       "    'angry': 1.7935622208044266,\n",
       "    'fear': 0.6984753632872863,\n",
       "    'happy': 0.16496183013998506,\n",
       "    'surprise': 0.009064874404292624,\n",
       "    'disgust': 0.0005716160331550266}}],\n",
       " [{'frame_00000': {'fear': 93.7836416025589,\n",
       "    'angry': 2.5416516462979737,\n",
       "    'surprise': 1.4251597389388395,\n",
       "    'sad': 1.2292429270752756,\n",
       "    'neutral': 0.9411941091782916,\n",
       "    'disgust': 0.07231172942006833,\n",
       "    'happy': 0.006796556325900173}},\n",
       "  {'frame_00001': {'fear': 88.16028237342834,\n",
       "    'neutral': 8.901528269052505,\n",
       "    'sad': 2.6532063260674477,\n",
       "    'angry': 0.14215297996997833,\n",
       "    'surprise': 0.1132711535319686,\n",
       "    'happy': 0.029248723876662552,\n",
       "    'disgust': 0.00031092135941435117}},\n",
       "  {'frame_00002': {'sad': 31.484286873596123,\n",
       "    'fear': 27.703853112688847,\n",
       "    'neutral': 27.461566177699932,\n",
       "    'angry': 13.053899291390138,\n",
       "    'happy': 0.12325737195878458,\n",
       "    'surprise': 0.11875078324233193,\n",
       "    'disgust': 0.054387897002348126}}],\n",
       " [{'frame_00000': {'fear': 94.788688110631,\n",
       "    'surprise': 3.0304178539167554,\n",
       "    'happy': 1.7253511682362062,\n",
       "    'neutral': 0.39408241884901635,\n",
       "    'sad': 0.054541000621974506,\n",
       "    'angry': 0.006919784621909461,\n",
       "    'disgust': 2.8001258537753284e-07}},\n",
       "  {'frame_00001': {'fear': 99.59995746612549,\n",
       "    'sad': 0.16049310797825456,\n",
       "    'neutral': 0.15582883497700095,\n",
       "    'surprise': 0.0533300859387964,\n",
       "    'angry': 0.02449218009132892,\n",
       "    'happy': 0.00586971509619616,\n",
       "    'disgust': 3.376214010586409e-05}}],\n",
       " [{'frame_00000': {'neutral': 98.11350107192993,\n",
       "    'sad': 1.6658484935760498,\n",
       "    'fear': 0.1298804534599185,\n",
       "    'happy': 0.07472079014405608,\n",
       "    'angry': 0.015626769163645804,\n",
       "    'surprise': 0.00042088504415005445,\n",
       "    'disgust': 1.5227957783636725e-08}},\n",
       "  {'frame_00001': {'sad': 60.01809597123241,\n",
       "    'neutral': 38.86103861695758,\n",
       "    'fear': 1.0969212932539758,\n",
       "    'angry': 0.01724433489482611,\n",
       "    'happy': 0.006461465476112245,\n",
       "    'surprise': 0.00024047220973405888,\n",
       "    'disgust': 3.030577747312639e-07}},\n",
       "  {'frame_00002': {'happy': 92.76606354308886,\n",
       "    'neutral': 5.348281454835112,\n",
       "    'angry': 0.8373843572895953,\n",
       "    'sad': 0.4294540313725509,\n",
       "    'fear': 0.33116619325175206,\n",
       "    'surprise': 0.28425272217425374,\n",
       "    'disgust': 0.0033988032059714293}},\n",
       "  {'frame_00003': {'happy': 95.11984586715698,\n",
       "    'neutral': 4.781676456332207,\n",
       "    'angry': 0.04679462581407279,\n",
       "    'surprise': 0.03778515674639493,\n",
       "    'sad': 0.013018709432799369,\n",
       "    'fear': 0.000884741257323185,\n",
       "    'disgust': 5.561364257999912e-07}}],\n",
       " [{'frame_00000': {'neutral': 58.566731214523315,\n",
       "    'fear': 27.328819036483765,\n",
       "    'sad': 11.509905010461807,\n",
       "    'angry': 1.7342887818813324,\n",
       "    'happy': 0.8484354242682457,\n",
       "    'disgust': 0.009001696889754385,\n",
       "    'surprise': 0.0028212003599037416}},\n",
       "  {'frame_00001': {'happy': 100.0,\n",
       "    'neutral': 2.148722266781533e-06,\n",
       "    'surprise': 1.0003597950003495e-07,\n",
       "    'sad': 4.3775873971216815e-13,\n",
       "    'angry': 3.672218698592413e-14,\n",
       "    'fear': 1.3082445583833748e-17,\n",
       "    'disgust': 3.2403034044258407e-28}},\n",
       "  {'frame_00002': {'fear': 94.7308835226824,\n",
       "    'sad': 2.7981551169249483,\n",
       "    'angry': 2.183684278052108,\n",
       "    'happy': 0.2110300245749965,\n",
       "    'neutral': 0.07602175917978639,\n",
       "    'surprise': 0.0001132052442872181,\n",
       "    'disgust': 0.00010847106260258402}}],\n",
       " [{'frame_00000': {'happy': 98.12499903142464,\n",
       "    'neutral': 1.796728344651658,\n",
       "    'sad': 0.07172552013689039,\n",
       "    'fear': 0.005999261351157489,\n",
       "    'angry': 0.0003393685397152574,\n",
       "    'surprise': 0.00020397223876439122,\n",
       "    'disgust': 2.414687883783698e-08}},\n",
       "  {'frame_00001': {'happy': 84.01786589728987,\n",
       "    'fear': 6.7232791616487,\n",
       "    'angry': 6.631768145070562,\n",
       "    'neutral': 1.1501141578027103,\n",
       "    'sad': 0.9353358856043748,\n",
       "    'surprise': 0.5335261431948032,\n",
       "    'disgust': 0.008114520944030342}}],\n",
       " [{'frame_00000': {'angry': 34.80638563632965,\n",
       "    'sad': 31.263214349746704,\n",
       "    'fear': 29.637205600738525,\n",
       "    'disgust': 1.8023926764726639,\n",
       "    'neutral': 1.6101548448204994,\n",
       "    'surprise': 0.8776683360338211,\n",
       "    'happy': 0.002980701538035646}},\n",
       "  {'frame_00001': {'fear': 72.95948701855463,\n",
       "    'sad': 19.55167916725154,\n",
       "    'neutral': 5.413160423228766,\n",
       "    'angry': 1.5317911263877826,\n",
       "    'surprise': 0.5292972865281872,\n",
       "    'disgust': 0.00926819248652488,\n",
       "    'happy': 0.005318616557416003}},\n",
       "  {'frame_00002': {'neutral': 85.76007996559619,\n",
       "    'sad': 10.416307921189542,\n",
       "    'angry': 2.4883448698130173,\n",
       "    'fear': 1.250477861641518,\n",
       "    'surprise': 0.07274916712748511,\n",
       "    'happy': 0.012024895102571032,\n",
       "    'disgust': 1.3999238869886647e-05}},\n",
       "  {'frame_00003': {'fear': 45.002397894859314,\n",
       "    'sad': 23.420384526252747,\n",
       "    'neutral': 22.35015481710434,\n",
       "    'angry': 8.583581447601318,\n",
       "    'happy': 0.6139429286122322,\n",
       "    'surprise': 0.020470170420594513,\n",
       "    'disgust': 0.00906670611584559}},\n",
       "  {'frame_00004': {'sad': 76.31624341011047,\n",
       "    'angry': 11.731430888175964,\n",
       "    'neutral': 7.3979102075099945,\n",
       "    'fear': 4.478611424565315,\n",
       "    'happy': 0.03220534126739949,\n",
       "    'disgust': 0.024649518309161067,\n",
       "    'surprise': 0.01894961460493505}}],\n",
       " [{'frame_00000': {'happy': 100.0,\n",
       "    'neutral': 4.435105793731964e-06,\n",
       "    'surprise': 2.1289516816125342e-07,\n",
       "    'sad': 1.4255397806205855e-11,\n",
       "    'angry': 1.3934842591888791e-11,\n",
       "    'fear': 2.2428971290102375e-16,\n",
       "    'disgust': 3.1360292840796517e-25}},\n",
       "  {'frame_00001': {'fear': 93.34263761893735,\n",
       "    'sad': 3.8268262938753543,\n",
       "    'happy': 1.417990858435086,\n",
       "    'neutral': 1.1361200108301646,\n",
       "    'angry': 0.26826683168213555,\n",
       "    'surprise': 0.00806689834341546,\n",
       "    'disgust': 9.918508703794383e-05}}],\n",
       " [{'frame_00000': {'fear': 90.21501657783787,\n",
       "    'sad': 9.446470803443098,\n",
       "    'angry': 0.16876900652196924,\n",
       "    'neutral': 0.09879237027333733,\n",
       "    'surprise': 0.04818430178091803,\n",
       "    'happy': 0.02267953261921166,\n",
       "    'disgust': 8.689944580502546e-05}},\n",
       "  {'frame_00001': {'neutral': 91.20168790797139,\n",
       "    'happy': 3.35971561256461,\n",
       "    'sad': 2.9825855216281982,\n",
       "    'angry': 1.4731696913338075,\n",
       "    'fear': 0.5817129514559565,\n",
       "    'surprise': 0.4008052858495372,\n",
       "    'disgust': 0.0003246118762512853}}]]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = new_process(segments)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feaa64",
   "metadata": {},
   "source": "# TEST GCLOUD "
  },
  {
   "cell_type": "code",
   "id": "945caafe",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import base64\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from ultralytics import YOLO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "123cf9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T01:42:54.216465Z",
     "start_time": "2024-07-08T01:42:54.213481Z"
    }
   },
   "source": [
    "def download_model_file():\n",
    "    bucket_name = 'last_model'\n",
    "    model_name = 'model.pt'\n",
    "    \"\"\"\n",
    "    Download the model .pt weights file.\n",
    "    \"\"\"\n",
    "    # bucket_name, artifacts_path = extract_from_uri(uri)\n",
    "    # logger.info(f\"Downloading model file from bucket: {bucket_name} and artifact path: {artifacts_path}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(model_name)\n",
    "    blob.download_to_filename(model_name)\n",
    "    \n",
    "    model = YOLO(model_name)\n",
    "    return model\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "20a8137f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T01:43:00.068357Z",
     "start_time": "2024-07-08T01:42:56.347733Z"
    }
   },
   "source": [
    "model = download_model_file()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7fb6b459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T01:43:00.073479Z",
     "start_time": "2024-07-08T01:43:00.069363Z"
    }
   },
   "source": [
    "type(model)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.models.yolo.model.YOLO"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf3c0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image: np.ndarray) -> Dict[str, float]:\n",
    "    try:\n",
    "        objs = DeepFace.analyze(image, actions=['emotion'])\n",
    "        results = objs[0]['emotion']\n",
    "        emotions = {k: v for k, v in sorted(results.items(), key=lambda x: x[1], reverse=True)}\n",
    "    except ValueError:\n",
    "        emotions = {'No face detected': 0.0}\n",
    "    return emotions\n",
    "\n",
    "def predict2(image: np.ndarray, predict_endpoint: aiplatform.Endpoint) -> Dict[str, float] | None:\n",
    "    try:\n",
    "        _, buffer = cv2.imencode('.jpg', image)\n",
    "        image_bytes = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "        # objs = DeepFace.analyze(image, actions=['emotion'])\n",
    "        response = predict_endpoint.predict(instances=[\n",
    "            {\n",
    "                \"image\": image_bytes\n",
    "            }\n",
    "        ])\n",
    "        results = response.predictions[0]\n",
    "        \n",
    "        \n",
    "        # results = objs[0]['emotion']\n",
    "        emotions = {k: v*100 for k, v in sorted(results.items(), key=lambda x: x[1], reverse=True)}\n",
    "        return emotions\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # emotions = {'No face detected': 0.0}\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce3db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"annual-project-427112\", location=\"europe-west1\")\n",
    "endpoint_name = None\n",
    "for endpoint in aiplatform.Endpoint.list():\n",
    "    if endpoint.display_name == \"yolo_predict\":\n",
    "        endpoint_name = endpoint.name\n",
    "predict_endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3eaac0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentiments from video\n",
      "Getting file raw.mp4 from the S3 bucket\n",
      "400 Endpoint projects/492916107091/locations/europe-west1/endpoints/7866772600272191488 misconfigured, \"traffic_split\" not set.  Verify if any models are deployed to the endpoint and traffic split is configured for them.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'emotions' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 50\u001B[0m\n\u001B[0;32m     48\u001B[0m img_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img)\n\u001B[0;32m     49\u001B[0m res1 \u001B[38;5;241m=\u001B[39m predict(img_array)\n\u001B[1;32m---> 50\u001B[0m res2 \u001B[38;5;241m=\u001B[39m \u001B[43mpredict2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredict_endpoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(res1)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(res2)\n",
      "Cell \u001B[1;32mIn[50], line 29\u001B[0m, in \u001B[0;36mpredict2\u001B[1;34m(image, predict_endpoint)\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(e)\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# emotions = {'No face detected': 0.0}\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43memotions\u001B[49m\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: cannot access local variable 'emotions' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "segments = get_segments_from_db()\n",
    "timing = 2.0\n",
    "\n",
    "all_sentiments = list()\n",
    "\n",
    "print('Processing sentiments from video')\n",
    "\n",
    "filename = 'raw.mp4'\n",
    "s3_path = '{}/{}/raw/{}'.format(session_id, interview_id, filename)\n",
    "video_bytes = open_input_file(s3_path, filename)\n",
    "\n",
    "container = av.open(io.BytesIO(video_bytes))\n",
    "\n",
    "for row in segments.itertuples():\n",
    "    sentiments = list()\n",
    "    image_count = 0\n",
    "    \n",
    "    start_time = row.start / 1000\n",
    "    end_time = row.end / 1000\n",
    "    \n",
    "    container.seek(int(start_time * av.time_base))\n",
    "    \n",
    "    # Flag to ensure end_time frame is captured\n",
    "    end_frame_captured = False\n",
    "    \n",
    "    # Initialize variables to track frame extraction\n",
    "    last_extracted_time = start_time - timing  # ensures the first frame is extracted at start_time\n",
    "    \n",
    "    for frame in container.decode(video=0):\n",
    "        frame_time = frame.time\n",
    "    \n",
    "        if frame_time < start_time:\n",
    "            continue\n",
    "        \n",
    "        # Extract the last frame at end_time\n",
    "        if frame_time >= end_time and not end_frame_captured:\n",
    "            img = frame.to_image()\n",
    "            img_array = np.array(img)\n",
    "            sentiments.append({'frame_{:05d}'.format(image_count): predict(img_array)})\n",
    "            image_count += 1\n",
    "            \n",
    "            end_frame_captured = True\n",
    "            # print('Last frame captured: ', frame_time)\n",
    "            break\n",
    "        \n",
    "        if frame_time >= start_time and frame_time >= last_extracted_time + timing and frame_time < end_time:\n",
    "            img = frame.to_image()\n",
    "            img_array = np.array(img)\n",
    "            res1 = predict(img_array)\n",
    "            res2 = predict2(img_array, predict_endpoint)\n",
    "            print(res1)\n",
    "            print(res2)\n",
    "            break\n",
    "            # sentiments.append({'frame_{:05d}'.format(image_count): predict(img_array)})\n",
    "            # image_count += 1\n",
    "            # print('Capturing frame ', frame_time)\n",
    "            \n",
    "            # Update the last extracted time\n",
    "            last_extracted_time = frame_time\n",
    "                \n",
    "    # all_sentiments.append(sentiments)\n",
    "\n",
    "container.close()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
