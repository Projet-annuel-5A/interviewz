{
 "cells": [
  {
   "cell_type": "code",
   "id": "2c2dbfba45603d25",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T07:42:07.126900Z",
     "start_time": "2024-05-12T07:42:07.119902Z"
    }
   },
   "source": [
    "from transformers import AutoProcessor, AutoModelForAudioClassification, AutoConfig, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T07:42:10.206727Z",
     "start_time": "2024-05-12T07:42:08.756624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"Lajavaness/wav2vec2-lg-xlsr-fr-speech-emotion-recognition\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_path)"
   ],
   "id": "d146bc483bf352",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T07:42:10.919602Z",
     "start_time": "2024-05-12T07:42:10.697457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
    "sampling_rate = feature_extractor.sampling_rate"
   ],
   "id": "2087e89d0da1c8c8",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T07:42:35.212046Z",
     "start_time": "2024-05-12T07:42:34.799460Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "70f02be0f5a747b7",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T05:50:15.988658Z",
     "start_time": "2024-05-12T05:50:15.971660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def speech_file_to_array_fn(path, sampling_rate):\n",
    "    speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(_sampling_rate, sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech"
   ],
   "id": "d77972004ec662ec",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T06:19:52.553706Z",
     "start_time": "2024-05-12T06:19:52.535707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    #outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    outputs = [(config.id2label[i], score*100) for i, score in enumerate(scores)]\n",
    "    return outputs"
   ],
   "id": "32ef973d89f49020",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T06:19:57.336872Z",
     "start_time": "2024-05-12T06:19:54.906152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = '../files/output/20240512/test/audio/speaker_000/part_00001.wav'\n",
    "outputs = predict(file_path, sampling_rate)\n",
    "outputs"
   ],
   "id": "9acf3aac4361ca50",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T05:50:28.706497Z",
     "start_time": "2024-05-12T05:50:26.318199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = '../files/output/20240512/test/audio/speaker_000/part_00001.wav'\n",
    "results = []\n",
    "\n",
    "outputs = predict(file_path, sampling_rate)\n",
    "for emotion in outputs:\n",
    "    emotion['Score'] = float(emotion['Score'].strip('%'))\n",
    "    results.append(tuple(emotion.values()))\n",
    "results"
   ],
   "id": "d29733de411568f2",
   "execution_count": 8,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
